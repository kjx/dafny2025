
@InProceedings{MPTP,
  author    = {James Noble and David Streader and Isaac Oscar Gariano and Miniruwani Samarakoon},
  title = {More Programming Than Programming: Teaching Formal Methods in a Software Engineering Programme},
  booktitle = {{NASA} Symposium on  Formal Methods},	       
  year      = 2022
}


@misc{wong2022exploring,
      title={Exploring the Verifiability of Code Generated by GitHub Copilot}, 
      author={Dakota Wong and Austin Kothig and Patrick Lam},
      year={2022},
      note={{HATRA} workshop @ {OOPSLA}},
      eprint={2209.01766},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
} 


@INPROCEEDINGS{siegmunds,
  author={Siegmund, Janet and Siegmund, Norbert and Apel, Sven},
  booktitle={International Conference on Software Engineering {(ICSE)}}, 
  title={Views on Internal and External Validity in Empirical Software Engineering}, 
  year={2015},
  volume={1},
  number={},
  pages={9-19},
  doi={10.1109/ICSE.2015.24}}


@inproceedings{feldt2010validity,
  title={Validity threats in empirical software engineering research-an initial survey.},
  author={Feldt, Robert and Magazinius, Ana},
  booktitle={Software Engineering and Knowledge Engineering {(SEKE)}},
  pages={374--379},
  year={2010}
}

@TechReport{sigplanEmpirical18,
  author =       {Emery Berger and  Steve Blackburn and Matthias Hauswirth and Michael Hicks},
  title =        {{SIGPLAN Empirical Evaluation Checklist}},
  institution =  {{ACM SIGPLAN EC}},
  year =         2018,
  note =         {www.sigplan.org/\-Resources/\-Empirical\-Evaluation}}



@misc{bach2022,
      title={PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
      author={Stephen H. Bach and Victor Sanh and Zheng-Xin Yong and Albert Webson and Colin Raffel and Nihal V. Nayak and Abheesht Sharma and Taewoon Kim and M Saiful Bari and Thibault Fevry and Zaid Alyafeai and Manan Dey and Andrea Santilli and Zhiqing Sun and Srulik Ben-David and Canwen Xu and Gunjan Chhablani and Han Wang and Jason Alan Fries and Maged S. Al-shaibani and Shanya Sharma and Urmish Thakker and Khalid Almubarak and Xiangru Tang and Xiangru Tang and Mike Tian-Jian Jiang and Alexander M. Rush},
      year={2022},
      eprint={2202.01279},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}




@misc{dang2022prompt,
      title={How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models}, 
      author={Hai Dang and Lukas Mecke and Florian Lehmann and Sven Goller and Daniel Buschek},
      year={2022},
      eprint={2209.01390},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@inproceedings{deckers2023,
author = {Deckers, Niklas and Fr\"{o}be, Maik and Kiesel, Johannes and Pandolfo, Gianluca and Schr\"{o}der, Christopher and Stein, Benno and Potthast, Martin},
title = {The Infinite Index: Information Retrieval on Generative Text-To-Image Models},
year = {2023},
isbn = {9798400700354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576840.3578327},
doi = {10.1145/3576840.3578327},
abstract = {Conditional generative models such as DALL-E and Stable Diffusion generate images based on a user-defined text, the prompt. Finding and refining prompts that produce a desired image has become the art of prompt engineering. Generative models do not provide a built-in retrieval model for a user’s information need expressed through prompts. In light of an extensive literature review, we reframe prompt engineering for generative models as interactive text-based retrieval on a novel kind of “infinite index”. We apply these insights for the first time in a case study on image generation for game design with an expert. Finally, we envision how active learning may help to guide the retrieval of generated images.},
booktitle = {Proceedings of the 2023 Conference on Human Information Interaction and Retrieval},
pages = {172–186},
numpages = {15},
keywords = {generative models, image retrieval, case study, evaluation},
location = {Austin, TX, USA},
series = {CHIIR '23}
}

@inproceedings{hou-etal-2022-metaprompting,
    title = "{M}eta{P}rompting: Learning to Learn Better Prompts",
    author = "Hou, Yutai  and
      Dong, Hongyuan  and
      Wang, Xinghao  and
      Li, Bohan  and
      Che, Wanxiang",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.287",
    pages = "3251--3262",
    abstract = "Prompting method is regarded as one of the crucial progress for few-shot nature language processing. Recent research on prompting moves from discrete tokens based {``}hard prompts{''} to continuous {``}soft prompts{''}, which employ learnable vectors as pseudo prompt tokens and achieve better performance. Though showing promising prospects, these soft-prompting methods are observed to rely heavily on good initialization to take effect. Unfortunately, obtaining a perfect initialization for soft prompts requires understanding of inner language models working and elaborate design, which is no easy task and has to restart from scratch for each new task. To remedy this, we propose a generalized soft prompting method called MetaPrompting, which adopts the well-recognized model-agnostic meta-learning algorithm to automatically find better prompt initialization that facilitates fast adaptation to new prompting tasks. Extensive experiments show MetaPrompting tackles soft prompt initialization problem and brings significant improvement on three different datasets (over 7 points improvement in accuracy for 1-shot setting), achieving new state-of-the-art performance.",
}

@inproceedings{jiang2022,
author = {Jiang, Ellen and Olson, Kristen and Toh, Edwin and Molina, Alejandra and Donsbach, Aaron and Terry, Michael and Cai, Carrie J},
title = {{PromptMaker}: Prompt-Based Prototyping with Large Language Models},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503564},
doi = {10.1145/3491101.3503564},
abstract = {Prototyping is notoriously difficult to do with machine learning (ML), but recent advances in large language models may lower the barriers to people prototyping with ML, through the use of natural language prompts. This case study reports on the real-world experiences of industry professionals (e.g. designers, program managers, front-end developers) prototyping new ML-powered feature ideas via prompt-based prototyping. Through interviews with eleven practitioners during a three-week sprint and a workshop, we find that prompt-based prototyping reduced barriers of access by substantially broadening who can prototype with ML, sped up the prototyping process, and grounded communication between collaborators. Yet, it also introduced new challenges, such as the need to reverse-engineer prompt designs, source example data, and debug and evaluate prompt effectiveness. Taken together, this case study provides important implications that lay the groundwork toward a new future of prototyping with ML.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {35},
numpages = {8},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{liu2022,
author = {Liu, Vivian and Chilton, Lydia B},
title = {Design Guidelines for Prompt Engineering Text-to-Image Generative Models},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501825},
doi = {10.1145/3491102.3501825},
abstract = {Text-to-image generative models are a new and powerful way to generate visual artwork. However, the open-ended nature of text as interaction is double-edged; while users can input anything and have access to an infinite range of generations, they also must engage in brute-force trial and error with the text prompt when the result quality is poor. We conduct a study exploring what prompt keywords and model hyperparameters can help produce coherent outputs. In particular, we study prompts structured to include subject and style keywords and investigate success and failure modes of these prompts. Our evaluation of 5493 generations over the course of five experiments spans 51 abstract and concrete subjects as well as 51 abstract and figurative styles. From this evaluation, we present design guidelines that can help people produce better outcomes from text-to-image generative models.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {384},
numpages = {23},
keywords = {text-to-image, prompt engineering., computational creativity, AI co-creation, multimodal generative models, design guidelines},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=\_VjQlMeSB\_J}
}

@article{mbpp,
  author       = {Jacob Austin and
                  Augustus Odena and
                  Maxwell I. Nye and
                  Maarten Bosma and
                  Henryk Michalewski and
                  David Dohan and
                  Ellen Jiang and
                  Carrie J. Cai and
                  Michael Terry and
                  Quoc V. Le and
                  Charles Sutton},
  title        = {Program Synthesis with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2108.07732},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.07732},
}


