\section{Background}

\subsection{Local Stores}

Aliasing is endemic in object-oriented programming
\cite{noble_flexible_1998}, and has long been recognised as a problem,
both in practice and in theory \cite{geneva91}.

Mark Utting \cite{utting1995} describes how formal techniques for
modelling and addressing pointer aliasing were developed by Strachey
\cite{strachey-varieties-book1997} and Hoare and Wirth
\cite{hoare-wirth-axiomatic-actainformatica1973}.  Key to this
approach is to treat all pointer-addressable memory, i.e.\ the heap in
a language with dynamical memory allocation, as a single large array
indexed by addresses. This works as far as it goes, but doesn't
address the ``aliasing problem'', or rather the problem of framing
computations involving pointers --- every pointer in a program could
potentially alias any other pointer, so a write through a pointer can
potentially invalidate every other fact about every other pointer and
the contents of the entire memory.  This makes most reasoning
infeasible.

In type-safe languages without low-level memory accesses, common
blocks, etc, dynamic memory can be modelled with an array for each
type of dynamically allocated objects, rather than one array for the
whole heap \cite{wirth-pldesign-ifip1974}. This is a measure of
progress: a pointer invalidates only all other pointers of the same
type.

Local stores take this design one step further: dividing up memory
(either within or across classes) into a series of smaller arrays
(arenas, regions, scopes) under programmer control \cite{utting1995}.
Local stores thus enable programmers to partition memory to better
reflect their program's design, so, rather than considering effects on
all pointers, or all pointers of a given type, any potential effects
can now be localised to the pointers within a given store.

Within programming language designs, local stores go back at least to
Wirth's early designs for Pascal \cite{wirth-pascal-1971}.  In
parallel with standard Pascal's FILE type, which grants
record-at-a-time access to an unbounded number of instances of a
subordinate component type, this earlier design had a CLASS type that
allocated space (presumably on the stack) for a maximum number of
dynamically allocated instances of a component type. Pointer variables
indexed a particular class, rather than all dynamically allocated
instances of a given type.


This design was built on in the Euclid language, where ``classes''
were renamed as ``collections'' \cite{euclid1977}. These collections
no longer have a fixed size, however in an interesting twist, they may
specify a maximum reference count for the objects they contain. Note
that objects in a collection may refer to objects within any other
colleciton in scope, so reference counting must still be global,
across all allocated objects.  Euclid collections are responsible for
the allocation and deallocation of the objects they contain: in this
sense, they act as stack-allocated \textit{regions} for memory
allocation \cite{smallmemory}. On the other hand, 
collections in Euclid are not first class:
``\textit{A collection may not be assigned to another collection. In
fact, there is nothing to do with a collection except to subscript it, or to pass it as a var actual
parameter. A collection type may not be the component type of an array or record or the object
type of a collection.}''
\cite{euclid1977}.


\subsection{Regions}

Gay and Aiken~\cite{explicitRegions1998} introduced explicit
regions for managing memory in C-like langauges: objects are
allocated in regions; and entire regions of objects are deleted in
one operation, rather than deleting objects individually. A later
version of this scheme added annotations to indicate that a
pointer refers to an object in the same region, in an enclosing
region, or is not allocated via the region
system~\cite{gay-regions-pldi2001}.

Rather than using explicit, programmer-visible regions, Tofte and
Talpin~\cite{tofte_region-based_1997, tofte-retrospective-2004}
demonstrated how Milner-style inference~\cite{baker-unify1990}
could be extended to implicitly allocate objects to regions, and
allocate and deallocate those regions, without either explicit
first-class regions, or additional annotations on programs or
types. Their MLKit~\cite{mlkit-4.6.0} runs ML programs using stack
allocation, as the regions are allocated last-in, first-out.
Because these inferred regions are implicit, the region structure
cannot capture a programmer's intent about how and where memory
should be allocated and freed: rather regions will always be inferred
to conform to the way regions --- or rather pointers --- are used.
This is in contrast to e.g.\ Euclid collections 
or Gay and Aitken's regions, where the region structure is described
explicitly, and programs are statically prevented from using pointers
in ways that conflict with the region structure.
MLKit remains under continuous
development, showing how regions can be supported in a monadic
style~\cite{fluet-monadic-jfp2006}, and integrating
generational-style GC within
regions~\cite{elsman-generational-regions-jfp2021}.
% \TODO{Note unsoundness bug and reference that paper.}

Safe region allocation was tested in
Cyclone~\cite{grossman_region-based_2002} an extension of C
with an explicit region construct, rather than using
inference. Like the MLKit, Cyclone regions were originally stack
based; later versions added support for unique pointers and
reference counted objects to permit deallocation of individual
objects inside a region, at the cost or introducing memory leaks
due to cycles or failure to deallocate a dropped unique
pointer~\cite{cyclone-ismm2004}.

Both MLKit style implicit /
inferred regions, and Cyclone explicit / annotated regions can be
modelled by a common core calculus based on linear references to
explicit, first-class
regions~\cite{fluet-linear-regions-esop2006}.




\subsection{Ownership and Structured Heaps}

Many 
contemporary programming languages such as
Rust~\cite{RustBook}, Pony~\cite{PonyTS},
Encore~\cite{EncoreTS},
%Singularity~\cite{Singularity},
%Deterministic Parallel Java~\cite{DPJ},
%Safe C\verb+#+~\cite{GordonPPBD12},
Obsidian~\cite{aldrichObsidianStudy2020}, and
Verona~\cite{Verona} 
have demonstrated the efficacy of \textit{static
  ownership}~\cite{ClaPotNobOOPSLA98,noble_flexible_1998} %%% BoyLeeRinOOPSLA02,ClaPotNobOOPSLA98 
%and capability systems
to ensure
concurrent programs are safe: Rust in particular has been adopted by
Microsoft~\cite{RustPopular,MSRust}.

By keeping track of each object's
ownership, these languages can determine when an object may be used,
when it may be changed, and the effects those changes can have on the
rest of the program. 


While much simpler than full-scale program proof systems, these 
languages rely on complicated static (compile-time) rules and
restrictions, with many different capability annotations and ownership
parameterisations that many programmers often find hard to learn and use
correctly~\cite{LearnRust,VizRust,HardRust}: they support writing
 correct and efficient programs, but they are still hard to understand
~\cite{SafeRust,FightRust} for a number of
reasons. First, their design must be conservative, banning not just
all  programs that are \textit{actually} unsafe, but a large
number of correct programs as well.  To programmers, this
manifests as a large number of \textit{false positive} errors or
warnings about problems that will never arise in practice.  For
example, Rust's version of ownership types~\cite{RustBook} bans even
such common idioms such as circular or doubly-linked lists.  Second,
programmers typically have to annotate their programs to give the
ownership and capability checkers the information they need --- so
rather than declaring an input stream ``\verb+in+'', programmers need
to write complex expressions such as ``\verb+in : &mut InStream<'a>+''
(where ``\verb+&mut+'' indicates that ``\verb+in+'' is a mutable
reference, ``\verb+InStream+'' indicates that ``\verb+in+'' refers to
an input stream, and  ``\verb+<'a>+'' is a lifetime (aka ownership)
parameter indicating the originating scope of the input stream.
Finally, these ownership annotations are required throughout
the program,  even if only a small part is actually concurrent, or is
otherwise likely to cause critical errors --- in Rust, an inflight
entertainment system would have to be engineered to the same level of
quality as a critical flight control system, even though the risks and
requirements for each system are very different.

More recently, language designers from various different traditions
have experimented with finding ways to make ownership systems easier
to use, often by relaxing the complex static restrictions required to
ensure safety at compile-time, by accepting some overhead for dynamic
checking to restore safety at run-time
\cite{dafnydala-ftfjp2024,reggio-oopsla2023,gallifrey-pldi2022}
or by employing more general solvers rather than type checkers to the
same end \cite{Astrauskas2019,linear-dafny-oopsla2022,prusti}.



Various varieties of ownership types have been used to support program
verification. Universe types require that modules own their own
invariants, so that their state can only be updated by the code within
the module
\cite{pubsdoc:universe-types-encapsulation,DietlMueller05,MuellerPoetzsch-HeffterLeavens06}.
Spec$\sharp$ uses a solver to track a conceptually similar notion of
ownership, again to track when modifications may invalidate an
invariant --- a development of an earlier, more dynamic model where
a solver uses ownership to ensure aggregate objects invariants are
consistent \cite{verification-oo-invariants-jot2004}.

% dafny stuff (stolen from another paper, ideally should revise!)

\subsection{Dafny}
Dafny~\cite{dafnytsite} is a verification-aware strongly typed programming language 
(with local type inference, objects, and algebraic data types)
first developed at Microsoft Research (MSR)~\cite{microsoftresearch} and  currently supported by the Amazon Automated Reasoning research group~\cite{awsautomatedreasoning}. Dafny's toolchain includes translators for generating executable code in different target languages such as C\#, Java, JavaScript, Go and Python~\cite{dafny-github}. But the distinguishing feature of Dafny is that it supports code verification via design by contract. For that, it follows the framework of Floyd-Hoare-style ~\cite{DBLP:journals/cacm/Hoare69} program verification with preconditions, postconditions, loop invariants and other high-level formal proof synthesis features such as pure functions, predicates, lemmas, and automated proof by induction. Dafny employs explicit dynamic frames to determine which assertions may be invalidated by imperative updates: imperative methods must be annotated \lstinline+writes S+\ldots to gain permission to write to objects in the set \lstinline+S+ (or occasionally, \lstinline+writes S`f+\ldots to write only to field \lstinline+f+ of the objects in\lstinline+S+).

To develop a verified program, developers write Dafny code along with the specifications and annotations to reason about the correctness of their code. While coding in Visual Studio, the Dafny static program verifier instantly verifies the functional correctness based on developers defined specifications and annotations. Correctness means the absence of any runtime errors with respect to the formal specifications, which means that the code does what the developers specify it to do. In order to confirm that the developer  specifications holds, the Dafny program verifier first transforms the code  into an intermediate verification representation (IVR) known as Boogie IVR~\cite{le2011boogie} language that expresses the verification conditions into predicate calculus~\cite{DBLP:journals/software/Leino17}. Next, an SMT solver tries to prove the verification conditions. In this phase, the Dafny verifier is backed by an advanced SMT solver known as the Z3 theorem prover~\cite{Z3-tacas2008}. The validity of these verification conditions implies the correctness of the code under consideration~\cite{DBLP:journals/software/Leino17}. Sometimes, however, the Z3 solver cannot automatically reach the required proof even though such proof exists. In such cases, developer intervention is required to give more context in the specifications by writing auxiliary verification conditions in the form of functions, predicates, and lemmas. 

In recent years, Dafny has had major successes: Microsoft used Dafny to formally verify security libraries and kernels~\cite{DBLP:conf/sosp/KleinEHACDEEKNSTW09,DBLP:journals/sigops/ErbsenPGSC20}, distributed systems~\cite{ironfleet-sosp2015}, and concurrent programs~\cite{DBLP:conf/osdi/HawblitzelHLNPZZ14}; Intel is developing its hardware encryption library using Dafny~\cite{DBLP:journals/iacr/YangWCCY23}; ConsenSys successfully applied Dafny for their Ethereum Virtual Machine (EVM) verification~\cite{cassex-eth-fm2023}; Amazon implemented the Amazon Web Service (AWS) authorization and encryption logic in Dafny and deployed the DafSdafnyny-generated Java code into production~\cite{counterexamples-tacas2022}.
Dafny is under continuous development, and has also served as a
research tool. Linear Dafny, for example, enhances Dafny with linear
types \cite{linear-dafny-oopsla2022}.

We can compare Dafny with tools more commonly used to verify
programming language designs such as REDEX~\cite{PLTRedex} or
Coq~\cite{coq,coqbook}.  REDEX is a testing and exploration tool:
designers can express the semantics of a whole language, and test and
explore those semantics, but REDEX itself does not make a claim to
formal validation.  On the other hand, as a higher-order
dependently-typed total functional language, designers can use Coq to
both specify and prove entire languages --- but constructing such a
explicit proof can take a large amount of time, and an even larger
amount of expertise.  Superficially, Dafny's validation claims are
much weaker than Coq's, as Dafny's implementation (verification
condition generator, SMT solver) is not itself verified.  While a
proof in Coq is explicit in the syntax and must be manupiulated by the
programmer, such a "proof" as Dafny constructs is implicit: users
write specifications and programs, but it's Dafny's job to validate
them.


Dafny does not use ownership to manage aliasing --- rather it takes
its name from the theory of ``\textit{dynamic frames}''
\cite{dynamic-frames-fm2006,dynamic-frames-fac2011}.  A dynamic frame
--- originally named ``\textit{desmesnes}'' (Norman French, pronounced
as and cognate to English ``\textit{domain}'') \cite{wills91,wills92}.
A demesnes or dynamic frame is the set of objects upon which one
particular object (or its invariant) depends.  Methods making
modifications must have dynamic frame speciications that cover every
object that will be modified, and reject programs that attempt
modifications to objects outside the frame. Similarly, two operations
or objects whose frames are disjoint, cannot interfere with each
other.
